{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bcd054",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "import pickle\n",
    "from joblib import dump, load\n",
    "from interpretation_code import interpretation\n",
    "from k_fold_cv import k_fold\n",
    "\n",
    "\n",
    "\n",
    "class model_trainer:\n",
    "\n",
    "    def __init__(self, train_path, test_path, model_name):\n",
    "        self.train_df=self.load_dataset(train_path)\n",
    "        self.test_df=self.load_dataset(test_path)\n",
    "        self.model_name=self.load_model(model_name)\n",
    "        self.tokenizer=self.load_tokenizer(model_name)\n",
    "#         self.model_name.resize_token_embeddings(len(self.tokenizer))\n",
    "        \n",
    "\n",
    "    def load_model(self,model_name):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        biobert = BertModel.from_pretrained(model_name).to(self.device)\n",
    "        print(\"BioBERT model loaded\")\n",
    "        return biobert\n",
    "\n",
    "    def load_tokenizer(self,model_name):\n",
    "        tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "#         tokenizer.add_tokens([\"GeneSrc\", \"DiseaseTgt\", \"causative\", \"causal\", \"cause\", \"causing\", \"caused\"])\n",
    "        print(\"Tokenizer loaded\")\n",
    "        return tokenizer\n",
    "\n",
    "    def load_dataset(self,data_path):\n",
    "        df = pd.read_csv(data_path, delimiter='\\t')\n",
    "        return df\n",
    "    \n",
    "    def remove_stopwords(self,text):\n",
    "        #nltk.download('stopwords')\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "        return ' '.join(tokens)\n",
    "    \n",
    "    \n",
    "    def create_dataset(self):\n",
    "        # Apply the remove_stopwords function to the 'sentence' column\n",
    "        self.train_df['sentence'] = self.train_df['sentence'].apply(self.remove_stopwords)\n",
    "        self.test_df['sentence'] = self.test_df['sentence'].apply(self.remove_stopwords)\n",
    "\n",
    "        X_train = self.train_df['sentence'].tolist()\n",
    "        y_train = self.train_df['label'].tolist()\n",
    "\n",
    "        X_test = self.test_df['sentence'].tolist()\n",
    "        y_test = self.test_df['label'].tolist()\n",
    "        \n",
    "        print(\"Dataset created\")\n",
    "        \n",
    "        return X_train,y_train,X_test,y_test\n",
    "    \n",
    "    def get_specific_token_embeddings(self,sentence):\n",
    "        # 1. Tokenize the input sentence\n",
    "        inputs = self.tokenizer(sentence, return_tensors='pt', padding=True, truncation=True, max_length=512).to(self.device)\n",
    "\n",
    "        # 2. Find the indices of \"@GeneSrc\" and \"@DiseaseTgt$\"\n",
    "        tokenized_sentence = self.tokenizer.tokenize(sentence)\n",
    "        gene_src_token = self.tokenizer.tokenize(\"@GeneSrc$\")\n",
    "        disease_tgt_token = self.tokenizer.tokenize(\"@DiseaseTgt$\")\n",
    "\n",
    "        gene_src_indices = [i for i, token in enumerate(tokenized_sentence) if token in gene_src_token]\n",
    "        disease_tgt_indices = [i for i, token in enumerate(tokenized_sentence) if token in disease_tgt_token]\n",
    "\n",
    "        # Run the sentence through BioBERT\n",
    "        with torch.no_grad():\n",
    "            #print(self.model_name)\n",
    "            outputs = self.model_name(**inputs)\n",
    "        embeddings = outputs['last_hidden_state'][0]  # Extracting embeddings for the whole sentence\n",
    "\n",
    "        # 3. Retrieve the embeddings for the surrounding tokens\n",
    "        context_range = 2\n",
    "\n",
    "        def get_context_embeddings(indices):\n",
    "            context_embeddings = []\n",
    "            for idx in indices:\n",
    "                start = max(0, idx - context_range)\n",
    "                end = min(idx + context_range + 1, len(tokenized_sentence))\n",
    "                context = embeddings[start:end]\n",
    "                context_embeddings.append(context)\n",
    "            return torch.cat(context_embeddings).view(-1, 768)\n",
    "\n",
    "        gene_src_embeddings = get_context_embeddings(gene_src_indices)\n",
    "        disease_tgt_embeddings = get_context_embeddings(disease_tgt_indices)\n",
    "\n",
    "        # 4. Compute the average of the embeddings\n",
    "        avg_gene_src_embedding = torch.mean(gene_src_embeddings, dim=0)\n",
    "        avg_disease_tgt_embedding = torch.mean(disease_tgt_embeddings, dim=0)\n",
    "\n",
    "        combined_embedding = torch.cat([avg_gene_src_embedding, avg_disease_tgt_embedding], dim=0)\n",
    "        combined_embedding_np = combined_embedding.cpu().numpy().reshape(1, -1)  # Convert tensor to NumPy array and reshape to 2D\n",
    "\n",
    "        if np.isnan(combined_embedding_np).any():\n",
    "            print(sentence)\n",
    "\n",
    "        return combined_embedding_np\n",
    "\n",
    "        \n",
    "        \n",
    "    def generate_embeddings(self):\n",
    "        X_train,y_train,X_test,y_test=self.create_dataset()\n",
    "        try:\n",
    "            X_train_embeddings = np.vstack([self.get_specific_token_embeddings(sentence) for sentence in tqdm(X_train)])\n",
    "            X_test_embeddings = np.vstack([self.get_specific_token_embeddings(sentence) for sentence in tqdm(X_test)])\n",
    "           \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass\n",
    "        print(\"Embeddings generated\")\n",
    "        return X_train_embeddings,X_test_embeddings\n",
    "    \n",
    "    def drop_null_embeddings(self,X_train_embeddings,X_test_embeddings,y_train,y_test):\n",
    "        X_train_embeddings=pd.DataFrame(X_train_embeddings).dropna()\n",
    "        X_test_embeddings=pd.DataFrame(X_test_embeddings).dropna()\n",
    "\n",
    "        test_ind=[i for i in range(0,len(X_test_embeddings)) if i not in X_test_embeddings.index]\n",
    "        train_ind=[i for i in range(0,len(X_train_embeddings)) if i not in X_train_embeddings.index]\n",
    "        for i in train_ind:\n",
    "            y_train.pop(i)\n",
    "        for i in test_ind:\n",
    "            y_test.pop(i)\n",
    "            \n",
    "        return X_train_embeddings,X_test_embeddings,y_train,y_test\n",
    "    \n",
    "    def svm_classifiation(self, X_train_embeddings,y_train,X_test_embeddings,y_test):\n",
    "        print(\"Doing classification using SVM...\")\n",
    "        clf = SVC(kernel='poly', degree=10, probability=True, class_weight={0:1, 1:30}, C=10)\n",
    "        clf.fit(X_train_embeddings, y_train)\n",
    "\n",
    "        # Predict and Evaluate\n",
    "        y_pred = clf.predict(X_test_embeddings)\n",
    "        print(\"Results of SVM classifier are: \")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        \n",
    "        \n",
    "    def xg_boost_classification(self, X_train_embeddings,y_train,X_test_embeddings,y_test):\n",
    "        print(\"Doing classification using XG Boost...\")\n",
    "        \n",
    "        clf = xgb.XGBClassifier(scale_pos_weight=200, max_depth=40, learning_rate=0.2, n_estimators=100, gamma=0.3)\n",
    "        clf.fit(X_train_embeddings, y_train)\n",
    "\n",
    "        # Get probabilities\n",
    "        y_prob = clf.predict_proba(X_test_embeddings)\n",
    "\n",
    "        # Predict and Evaluate\n",
    "        y_pred = clf.predict(X_test_embeddings)\n",
    "        \n",
    "        # Save the model to a file\n",
    "#         dump(clf, 'CRED_trained_model_new_data.joblib') \n",
    "\n",
    "        print(\"Results of XG Boost are: \")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        \n",
    "    def random_forest(self, X_train_embeddings,y_train,X_test_embeddings,y_test):\n",
    "        print(\"Doing classification using Random Forest...\")\n",
    "        \n",
    "\n",
    "        # Instantiate the Random Forest Classifier\n",
    "        clf = RandomForestClassifier(n_estimators=100, max_depth=10, max_leaf_nodes= 500, n_jobs= 2, max_features=\"sqrt\", class_weight={0:1, 1:30}, verbose=True)\n",
    "        clf.fit(X_train_embeddings, y_train)\n",
    "\n",
    "        # Predict and Evaluate\n",
    "        y_pred = clf.predict(X_test_embeddings)\n",
    "        print(\"Results of Random Forest are: \")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        \n",
    "        \n",
    "    def interpretation_call(self):\n",
    "\n",
    "        causal_test_df=self.test_df[self.test_df[\"label\"]==1]\n",
    "        causal_df_unique = causal_test_df.drop_duplicates(subset=['index', 'id1', 'id2'])\n",
    "\n",
    "        samp_abst=causal_df_unique[causal_df_unique[\"index\"]==25064704]\n",
    "\n",
    "        # Load the model from a file\n",
    "        clf = load('/home/ubuntu/CRED_application/CRED_trained_model_new_data.joblib')\n",
    "\n",
    "        model_name = \"dmis-lab/biobert-base-cased-v1.1\"\n",
    "        biobert = BertModel.from_pretrained(model_name).to(self.device)\n",
    "        tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "        for _, row in tqdm(causal_df_unique.iterrows()):\n",
    "            ranked, word_importance = interpretation(row, clf, tokenizer, biobert, self.get_specific_token_embeddings)\n",
    "\n",
    "    \n",
    "    \n",
    "def main():       \n",
    "    mod_tr=model_trainer('new_train_data', 'test_data', \"dmis-lab/biobert-base-cased-v1.1\")\n",
    "    \n",
    "    #To generate embeddings uncomment the below line\n",
    "    X_train_embeddings,X_test_embeddings= mod_tr.generate_embeddings()\n",
    "    X_train,y_train,X_test,y_test = mod_tr.create_dataset()\n",
    "    \n",
    "#     Reading the training embeddings from the text file\n",
    "#     X_train_embeddings = pd.read_csv('/home/ubuntu/CRED_application/X_train_embeddings_new_data.txt', sep='\\t', header=None)\n",
    "#     X_test_embeddings = pd.read_csv('/home/ubuntu/CRED_application/X_test_embeddings.txt', sep='\\t', header=None)\n",
    "    X_train_embeddings,X_test_embeddings,y_train,y_test = mod_tr.drop_null_embeddings(X_train_embeddings,X_test_embeddings,y_train,y_test)\n",
    "    print(\"Embeddings generated\")\n",
    "    \n",
    "#     mod_tr.svm_classifiation(X_train_embeddings,y_train,X_test_embeddings,y_test)\n",
    "#     mod_tr.xg_boost_classification(X_train_embeddings,y_train,X_test_embeddings,y_test)\n",
    "#     mod_tr.random_forest(X_train_embeddings,y_train,X_test_embeddings,y_test)\n",
    "    \n",
    "    #For Interpretaion uncomment the below 2 lines\n",
    "#     print(\"Interpretaion started. It may take upto 10 minutes...\")\n",
    "#     mod_tr.interpretation_call()\n",
    "    \n",
    "    #For 4-fold cross validation uncomment the below 2 lines\n",
    "#     print(\"4-fold cross validation started. It may take few minutes...\")\n",
    "#     k_fold(mod_tr.train_df, mod_tr.test_df, mod_tr.get_specific_token_embeddings)\n",
    "\n",
    "\n",
    "#     # Saving the training embeddings to a text file\n",
    "#     pd.DataFrame(X_train_embeddings).to_csv('X_train_embeddings_without_extra_tokens.txt', sep='\\t', index=False, header=False)\n",
    "\n",
    "#     # Saving the test embeddings to a text file\n",
    "#     pd.DataFrame(X_test_embeddings).to_csv('val_embeddings_without_extra_tokens.txt', sep='\\t', index=False, header=False)\n",
    "\n",
    "    \n",
    "main()    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9671fdf7",
   "metadata": {},
   "source": [
    "# Code to generate embeddings with gda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3c5d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import wandb\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class model_trainer:\n",
    "    def __init__(self, train_df, test_df, val_df, model_name):\n",
    "        self.train_df = train_df\n",
    "        self.test_df = test_df\n",
    "        self.val_df = val_df\n",
    "        self.model_name = self.load_model(model_name)\n",
    "        self.tokenizer = self.load_tokenizer(model_name)\n",
    "\n",
    "    def load_model(self, model_name):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        biobert = BertModel.from_pretrained(model_name).to(self.device)\n",
    "        print(\"BioBERT model loaded\")\n",
    "        return biobert\n",
    "\n",
    "    def load_tokenizer(self, model_name):\n",
    "        tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        print(\"Tokenizer loaded\")\n",
    "        return tokenizer\n",
    "\n",
    "    def remove_stopwords(self, text):\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    def create_dataset(self):\n",
    "        self.train_df['sentence'] = self.train_df['sentence'].apply(self.remove_stopwords)\n",
    "        self.test_df['sentence'] = self.test_df['sentence'].apply(self.remove_stopwords)\n",
    "        self.val_df['sentence'] = self.val_df['sentence'].apply(self.remove_stopwords)\n",
    "\n",
    "        X_train = self.train_df[['sentence', 'gda_score']].values.tolist()\n",
    "        y_train = self.train_df['label'].tolist()\n",
    "\n",
    "        X_test = self.test_df[['sentence', 'gda_score']].values.tolist()\n",
    "        y_test = self.test_df['label'].tolist()\n",
    "       \n",
    "        X_val = self.test_df[['sentence', 'gda_score']].values.tolist()\n",
    "        y_val = self.test_df['label'].tolist()\n",
    "\n",
    "        print(\"Dataset created\")\n",
    "\n",
    "        return X_train, y_train, X_test, y_test, X_val, y_val\n",
    "\n",
    "    def get_specific_token_embeddings(self, sentence):\n",
    "        inputs = self.tokenizer(sentence, return_tensors='pt', padding=True, truncation=True, max_length=512).to(self.device)\n",
    "        tokenized_sentence = self.tokenizer.tokenize(sentence)\n",
    "        gene_src_token = self.tokenizer.tokenize(\"@GeneSrc$\")\n",
    "        disease_tgt_token = self.tokenizer.tokenize(\"@DiseaseTgt$\")\n",
    "\n",
    "        gene_src_indices = [i for i, token in enumerate(tokenized_sentence) if token in gene_src_token]\n",
    "        disease_tgt_indices = [i for i, token in enumerate(tokenized_sentence) if token in disease_tgt_token]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model_name(**inputs)\n",
    "        embeddings = outputs.last_hidden_state[0]\n",
    "\n",
    "        context_range = 2\n",
    "\n",
    "        def get_context_embeddings(indices):\n",
    "            context_embeddings = []\n",
    "            for idx in indices:\n",
    "                start = max(0, idx - context_range)\n",
    "                end = min(idx + context_range + 1, len(tokenized_sentence))\n",
    "                context = embeddings[start:end]\n",
    "                context_embeddings.append(context)\n",
    "            return torch.cat(context_embeddings).view(-1, 768)\n",
    "\n",
    "        gene_src_embeddings = get_context_embeddings(gene_src_indices)\n",
    "        disease_tgt_embeddings = get_context_embeddings(disease_tgt_indices)\n",
    "\n",
    "        avg_gene_src_embedding = torch.mean(gene_src_embeddings, dim=0)\n",
    "        avg_disease_tgt_embedding = torch.mean(disease_tgt_embeddings, dim=0)\n",
    "\n",
    "        combined_embedding = torch.cat([avg_gene_src_embedding, avg_disease_tgt_embedding], dim=0)\n",
    "        combined_embedding_np = combined_embedding.cpu().numpy().reshape(1, -1)\n",
    "\n",
    "        if np.isnan(combined_embedding_np).any():\n",
    "            print(sentence)\n",
    "\n",
    "        return combined_embedding_np\n",
    "\n",
    "    def generate_embeddings(self):\n",
    "        X_train, y_train, X_test, y_test, X_val, y_val = self.create_dataset()\n",
    "        try:\n",
    "            X_train_embeddings = []\n",
    "            for sentence, gda_score in tqdm(X_train):\n",
    "                sentence_embedding = self.get_specific_token_embeddings(sentence)\n",
    "                gda_score = np.array([[gda_score]], dtype=np.float32)  # Ensure gda_score is a numpy array with correct dtype\n",
    "                embedding_with_score_window = np.concatenate([sentence_embedding, gda_score], axis=1)\n",
    "#                 embedding_with_score_window = np.concatenate([sentence_embedding], axis=1)\n",
    "                X_train_embeddings.append(embedding_with_score_window)\n",
    "            X_train_embeddings = np.vstack(X_train_embeddings)\n",
    "\n",
    "            X_test_embeddings = []\n",
    "            for sentence, gda_score in tqdm(X_test):\n",
    "                sentence_embedding = self.get_specific_token_embeddings(sentence)\n",
    "                gda_score = np.array([[gda_score]], dtype=np.float32)  # Ensure gda_score is a numpy array with correct dtype\n",
    "                embedding_with_score_window = np.concatenate([sentence_embedding, gda_score], axis=1)\n",
    "#                 embedding_with_score_window = np.concatenate([sentence_embedding], axis=1)\n",
    "                X_test_embeddings.append(embedding_with_score_window)\n",
    "            X_test_embeddings = np.vstack(X_test_embeddings)\n",
    "           \n",
    "            X_val_embeddings = []\n",
    "            for sentence, gda_score in tqdm(X_val):\n",
    "                sentence_embedding = self.get_specific_token_embeddings(sentence)\n",
    "                gda_score = np.array([[gda_score]], dtype=np.float32)  # Ensure gda_score is a numpy array with correct dtype\n",
    "                embedding_with_score_window = np.concatenate([sentence_embedding, gda_score], axis=1)\n",
    "#                 embedding_with_score_window = np.concatenate([sentence_embedding], axis=1)\n",
    "                X_val_embeddings.append(embedding_with_score_window)\n",
    "            X_val_embeddings = np.vstack(X_val_embeddings)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass\n",
    "        print(\"Embeddings generated\")\n",
    "        return X_train_embeddings, X_test_embeddings, X_val_embeddings, y_train, y_test, y_val\n",
    "\n",
    "\n",
    "train_df = pd.read_csv('new_train_data_with_gda', delimiter='\\t')\n",
    "test_df = pd.read_csv('test_data_with_gda', delimiter='\\t')\n",
    "test_df.rename(columns={'gda_avg': 'gda_score'}, inplace=True)\n",
    "val_df = pd.read_csv('val_data_with_gda', delimiter='\\t')\n",
    "val_df.rename(columns={'gda_avg': 'gda_score'}, inplace=True)\n",
    "\n",
    "mod_tr=model_trainer(train_df, test_df, val_df, \"dmis-lab/biobert-base-cased-v1.1\")\n",
    "    \n",
    "    #To generate embeddings uncomment the below line\n",
    "X_train_embeddings, X_test_embeddings, X_val_embeddings, y_train, y_test, y_val=mod_tr.generate_embeddings() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
